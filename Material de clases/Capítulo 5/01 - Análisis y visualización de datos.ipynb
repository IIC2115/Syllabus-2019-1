{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<font size='5' face='Georgia, Arial'>IIC2115 - Programación como herramienta para la ingeniería</font><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herramientas para Python\n",
    "\n",
    "En python, así como en otros lenguajes, existe una infinidad de librerías o API (Application Programming Interface) que permiten al programador desarrollar tareas especifícas con gran facilidad. Algunas de las tareas que podemos llevar a cabo son:\n",
    "\n",
    "* Analizar y visualizar datos.\n",
    "* Mapear y trabajar con sistemas de información geográfica (GIS).\n",
    "* Enviar correos electrónicos, WhatsApp o realizar conxiones remotas.\n",
    "* Conectarse a dispositivos externos (como impresoras, celulares, cámaras, entre otras).\n",
    "* Vincular nuestro código con otros softwares dedicados como R, stata, Qgis.\n",
    "* Conectarse con servicios como Google Maps, Open Street Maps u otra página web o servicio en línea.\n",
    "* Conectarse a aplicaciones como Twitter o Waze.\n",
    "\n",
    "Estos son solo algunos ejemplos, pero existe un sin límite de herramientas disponibles. Para encontrarlas, es necesario realizar busquedas en la red, llegar a páginas de desarrollo de distintas plataformas como Google, Waze Developer o repositorios en GitHub. \n",
    "\n",
    "En este curso, veremos algunas herramientas con el fin de que aprendan a interiorizarse con su uso. Pero, vale decir que cada librería o API es diferente, desarrollada por personas diferentes por lo que es muy importante realizar una busqueda en la red y quedarse con las alternativas mejor documentadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Análisis y visualización de datos\n",
    "\n",
    "El análisis y visualización de datos es una herramienta sumamente útil a la hora de resolver problemas. Es por eso que será la primera que revisaremos. El análisis y visualización de datos es una combinación multidisciplinaria de inferencia de datos, desarrollo de algoritmos y tecnología para resolver problemas analíticamente complejos.\n",
    "\n",
    "A diferencia de otras disciplinas, en el centro de esta están los datos, por lo general grandes volúmenes de información sin procesar, transmitida y almacenada en bases de datos de distintos tipos. El objetivo principal se centra en extraer información valiosa de los datos, _buceando_ a un nivel granular para extraer y comprender comportamientos complejos, tendencias e inferencias. Esencialmente, se trata de revelar información oculta que pueda ayudar a tomar decisiones y acciones inteligentes, como las siguientes:\n",
    "\n",
    "* Netflix analiza en sus datos los patrones de visualización de películas para comprender qué impulsa el interés del usuario y lo usa para tomar decisiones sobre qué serie original producir.\n",
    "* Target (cadena de retail estadounidense) identifica cuáles son los principales segmentos de clientes dentro de su base de datos y los comportamientos únicos de compra dentro de esos segmentos, lo que ayuda a orientar la mensajería a diferentes tipos de público.\n",
    "* Procter & Gamble utiliza modelos de series de tiempo para comprender más claramente la demanda futura, lo que ayuda a planificar los niveles de producción de mejor manera.\n",
    "\n",
    "¿Cómo analizan los expertos los datos para extraer información? Todo comienza con la exploración de datos. Cuando se les presenta una pregunta desafiante, los analistas se convierten en algo similar a un detective, investigando pistas y tratando de comprender el patrón o las características dentro de los datos. Esto requiere una gran dosis de creatividad analítica.\n",
    "\n",
    "Luego, según sea necesario, los analistas de datos pueden aplicar una técnica cuantitativa para profundizar aún más en la información: modelos predictivos y generativos, segmentación, predicción de series de tiempo, etc. El objetivo es generar científicamente una visión general de lo que realmente dicen los datos.\n",
    "\n",
    "Esta información basada en datos es fundamental para proporcionar orientación estratégica. En este sentido, los analistas de datos actúan como consultores, guiando a las partes interesadas sobre cómo actuar ante los hallazgos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.- Herramientas para Data Science (en Python)\n",
    "\n",
    "A continuación se muestra una lista de bibliotecas que se utilizan regularmente para realizar cálculo científico, y análisis y visualización de datos en Python.\n",
    "\n",
    "* NumPy: es una biblioteca de cálculo científico. Contiene, entre otras cosas, funciones básicas de álgebra lineal, y capacidades avanzadas de generación de números aleatorios. Su característica más poderosa es el arreglo n-dimensional.\n",
    "* SciPy: basado en NumPy. Es una de las bibliotecas más útiles para una gran variedad de tópicos en ingeniería y ciencia. Implementa elementos como la transformada discreta de Fourier, álgebra lineal, optimización y matrices dispersas (sparse).\n",
    "* Matplotlib: se utiliza para generar una gran variedad de gráficos, desde histogramas hasta mapas de calor.\n",
    "* Seaborn: muy similar a Matplotlib pero con algunas diferencias en los tipos de gráficos que se pueden generar.\n",
    "* Pandas: permite realizar operaciones sobre datos estructurados. Se usa ampliamente para la manipulación y preparación de datos. Pandas se agregó recientemente a Python y ha sido fundamental para impulsar el uso de este en la comunidad de ciencia de datos.\n",
    "* Scikit Learn: basado en NumPy, SciPy y matplotlib, esta biblioteca contiene una gran cantidad de herramientas  para el aprendizaje de máquina y el modelamiento estadístico, que incluyen clasificación, regresión, clustering y reducción de dimensionalidad.\n",
    "\n",
    "Es recomendable buscar estas librerías en la red, entender un poco mejor su funcionamiento y cómo instalarlas en nuestro entorno. Muchas ya se encuentran instaladas y vienen por defecto en Python o Anaconda, pero otras hay que instalarlas manualmente para que funcionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- El proceso de análisis de datos\n",
    "\n",
    "Ahora que estamos familiarizados con los nombres y objetivos de las bibliotecas adicionales, realizaremos una revisión de la resolución de problemas de datos a través de Python. En particular, el objetivo de esta sección es construir un modelo predictivo eficaz, lo que nos llevará por los siguientes 3 etapas claves:\n",
    "\n",
    "* Análisis exploratorio de datos\n",
    "* Filtrado y depuración\n",
    "* Modelamiento predictivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.- Análisis exploratorio\n",
    "\n",
    "Para esta etapa del proceso, utiliaremos la librería Pandas. En particular, la utilizaremos para leer un conjunto de datos y realizar un análisis exploratorio. \n",
    "\n",
    "Primero vamos a corroborar que Pandas se encuentre instalado en nuestro equipo, para ello ejecutamos la siguiente línea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos entrega un error \"ModuleNotFoundError\" significa que no contamos con la librería, por lo que es necesario instalarla, ejecutando `pip`, que nos permite instalar librerías en Python. El formato de la sentencia es `pip install NOMBRE_LIBRERIA`. En este caso es `pip install pandas`. Este tipo de comandos no se ejecutan en Python, sino que en la consola/terminal de nuestro entorno. En el caso de Jupyter o Colab se utiliza el carácter exclamación \"!\" para introducir una secuencia de consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el comando `pip install` verás como se cargan las librerías necesarias o te informará que todo está en orden si es que ya cuentas con Pandas. Cabe destacar que no todas las librerías cuentan con la facilidad de ser instaladas mediante `pip install`. Además, algunas librerías que veremos más adelante, dependen de otras librerías, las que deben estar instaladas antes de instalar nuestra librería objetivo. Por eso, es muy importante informarse sobre este proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuando con el análisis exploratorio, antes de cargar los datos, vamos a describir las 2 estructuras de datos clave en Pandas: Series y DataFrames\n",
    "\n",
    "**Series** se puede entender como un arreglo unidimensional etiquetado/indexado. Se puede acceder a elementos individuales de esta Serie a través de estas etiquetas.\n",
    "\n",
    "**DataFrame** es similar a un libro de Excel, tiene nombres de columnas que hacen referencia a ellas y tiene filas, a las que se puede acceder mediante el uso de números de estas. La diferencia esencial es que acá, los nombres de columna y los números de fila se conocen como índice de columna y fila.\n",
    "\n",
    "Series y DataFrames forman el modelo de datos básicos para Pandas en Python. Los conjuntos de datos se leen primero en DataFrames y luego se pueden aplicar fácilmente varias operaciones (por ejemplo, agrupar por, agregación, etc.) a sus columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos para el ejemplo un set de datos de información de créditos bancarios que se encuentra en el archivo `train.csv`. Para importarlo, basta con utilizar la función *read_csv()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import display #para mostrar más de un elemento por celda de Jupyter\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "display(df.head(10))\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te ha salido otro error \"ModuleNotFoundError\" ya sabes que hacer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `describe()` proporciona el conteo, media, desviación, mínimo, cuartiles y máximo de los datos. A continuación algunas aspectos que pueden deducirse de esta información:\n",
    "\n",
    "* LoanAmount tiene (614-592) 22 valores perdidos.\n",
    "* Loan_Amount_Term tiene (614-600) 14 valores faltantes.\n",
    "* Credit_History tiene (614-564) 50 valores perdidos.\n",
    "* También podemos observar que alrededor del 84% de los solicitantes tienen un historial crediticio (la media del campo Credit_History es 0.84).\n",
    "\n",
    "Para los valores no numéricos (por ejemplo, Property_Area, Credit_History, etc.), podemos ver la distribución de frecuencias para comprender si tienen sentido o no. La tabla de frecuencias se puede imprimir con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Property_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo, podemos mirar los valores únicos del historial de crédito. Es importante tener en cuenta que `df['columna']` es una técnica de indexación básica para acceder a una columna particular del DataFrame. Puede ser una lista de columnas también."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis distribucional\n",
    "\n",
    "Ahora que estamos familiarizados con las características básicas de los datos, estudiemos la distribución de algunas de sus variables. Comencemos con las variables numéricas `ApplicantIncome` y `LoanAmount`, en particular, con el histograma de `ApplicantIncome` usando los siguientes comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ApplicantIncome'].hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí observamos que hay algunos valores extremos. Esta es también la razón por la cual se requieren 50 _bins_ para representar claramente la distribución.\n",
    "\n",
    "A continuación, revisaremos Box Plots para comprender las distribuciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column = 'ApplicantIncome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto gráfico indica la presencia de valores extremos más claramente que el histograma. Esto se puede atribuir a la disparidad de ingresos en la sociedad. Parte de esto puede ser impulsado por el hecho de que estamos viendo personas con diferentes niveles de educación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='ApplicantIncome', by = 'Education')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que no hay diferencias sustanciales entre los ingresos medios de los graduados y los no graduados. Pero hay un mayor número de graduados con ingresos muy altos, que parecen ser los valores atípicos.\n",
    "\n",
    "Ahora, veamos el histograma y el Box Plot de `LoanAmount`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount'].hist(bins=50)\n",
    "plt.show()\n",
    "df.boxplot(column='LoanAmount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente hay valores extremos. Con el fin de facilitar el modelamiento predictivo, tanto `ApplicantIncome` como `LoanAmount` requieren una cierta cantidad de depuración de datos (los valores extremos son difíciles de predecir). `LoanAmount` tiene además valores incompletos y también extremos, mientras que `ApplicantIncome` tiene algunos valores extremos, que exigen una comprensión más profunda. Continuaremos con el análisis de estas variables más adelante. A continuación revisaremos el análisis de variables categóricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de variables categóricas\n",
    "\n",
    "Ahora que conocemos las distribuciones para `ApplicantIncome` y `LoanIncome`, analicemos las variables categóricas en más detalle. Utilizaremos tablas dinámicas tipo Excel y tabulación cruzada. Es importante notar que aquí el estado del préstamo ha sido codificado como 1 para sí y 0 para no. Por lo tanto, la media representa la probabilidad de obtener un préstamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = df['Credit_History'].value_counts(ascending=True)\n",
    "print('Tabla de frecuencia para el historial de crédito:')\n",
    "print(temp1)\n",
    "\n",
    "temp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n",
    "print('\\nProbabilidad de obtener un crédito, en base a la existencia de historial crediticio:')\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas mismas tablas se pueden mostrar como un gráfico de barras usando la biblioteca *matplotlib* con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax1 = temp1.plot(kind='bar')\n",
    "ax1.set_xlabel('Credit_History')\n",
    "ax1.set_ylabel('Count of Applicants')\n",
    "ax1.set_title(\"Applicants by Credit_History\")\n",
    "\n",
    "\n",
    "ax2 = temp2.plot(kind = 'bar')\n",
    "ax2.set_xlabel('Credit_History')\n",
    "ax2.set_ylabel('Probability of getting loan')\n",
    "ax2.set_title(\"Probability of getting loan by credit history\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Esto muestra que las posibilidades de obtener un préstamo son ocho veces mayores si el solicitante tiene un historial crediticio válido.\n",
    "\n",
    "Alternativamente, estos dos gráficos también se pueden visualizar combinándolos en un gráfico apilado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible agregar la información de género:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp4 = pd.crosstab([df['Credit_History'],df['Gender']], df['Loan_Status'])\n",
    "temp4.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acabamos de ver cómo podemos hacer un análisis exploratorio en Python usando Pandas, lo que nos entregó información relevante que será utilizada en la siguiente etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interludio: funciones lambda\n",
    "Las funciones `lambda` son una forma alternativa de definir funciones en Python.\n",
    "Además de su nombre griego, no hay nada intimidante en ellas.\n",
    "Veamos un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumar_uno = lambda x: x+1\n",
    "\n",
    "#es (casi) equivalente a\n",
    "\n",
    "def sumar_uno(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para que el gráfico se genere dentro del notebook y no en una ventana aparte\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "gauss = lambda x, mu, sigma: (1./(np.sqrt(2*np.pi)*sigma)) * np.exp(-0.5*((x - mu)/sigma)**2)\n",
    "\n",
    "mu = 0.\n",
    "sigma = 0.2\n",
    "x = np.linspace(-2,2,300)\n",
    "plt.plot(x, gauss(x, mu, sigma), '.r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de lo anterior, las funciones `lambda` pueden ser definidas de forma anónima; es decir, funciones que no tienen nombre. Estas funciones pueden ser vistas como _fugaces_ y son utilizadas únicamente donde fueron creadas. Esta anonimidad se combina bien con las funciones que veremos a continuación: `map`, `filter`, `reduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `map`\n",
    "\n",
    "La función `map` aplica, en esencia, una misma función a todos los elementos de un objeto iterable (lista, diccionario, set, etc.). Recibe como parámetros una función y al menos un objeto iterable. Retorna un generador que resulta de aplicar la función sobre el iterable. `map(f, iterable)` es equivalente a `(f(x) for x in iterable)`\n",
    "\n",
    "La cantidad de iterables entregada a `map` debe corresponder con la cantidad de parámetros que recible la función `f`. Por ejemplo, `map(f, iterable1, iterable2)` es equivalente a `(f(x) for x in zip(iterable1, iterable2))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow2 = lambda x : x**2\n",
    "t = np.linspace(-1.,1., 100)#crea un arreglo numpy de 100 elementos, partiendo desde -1 y llegando a 1\n",
    "plt.plot(t, list(map(pow2, t)), 'xb')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map puede ser aplicado también en más de una lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "b = [17, 12, 11, 10]\n",
    "c = [-1, -4, 5, 9]\n",
    "\n",
    "c1 = list(map(lambda x, y: x + y, a, b))\n",
    "\n",
    "c2 = list(map(lambda x, y, z: x + y + z, a, b, c))\n",
    "\n",
    "c3 = list(map(lambda x, y, z: 2.5*x + 2*y - z, a, b, c))\n",
    "\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `filter`   \n",
    "\n",
    "`filter(f, secuencia)` retorna el resultado de aplicar la función `f` a `secuencia`, dejando fuera los datos en que el resultado de aplicar `f` al elemento fue `False`. La función `f` **debe** retornar un valor de tipo booleano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    a,b = 0,1\n",
    "    values = []\n",
    "    for i in range(1,n):\n",
    "        values.append(b)\n",
    "        a, b = b, a + b\n",
    "    return values\n",
    "        \n",
    "fib = fibonacci(11)\n",
    "impares = list(filter(lambda x: x % 2 != 0, fib))\n",
    "print(impares)\n",
    "\n",
    "pares = list(filter(lambda x: x % 2 == 0, fib))\n",
    "print(pares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `reduce`\n",
    "\n",
    "`reduce(f, [s1,s2,s3,...,sn])` retorna lo que resulta de aplicar la función `f` a la secuencia `[s1, s2, s3, ..., sn]` de la siguiente forma: `f(f(f(f(s1,s2),s3),s4),s5),...`  ![](figs/reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "reduce(lambda x, y: x+y, range(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.- Limpieza y depuración de los datos\n",
    "\n",
    "Mientras exploramos los datos, encontramos algunos problemas en estos, que deben resolverse antes de que estén listos para construir un modelo predictivo. Aquí algunos de los problemas de los que ya somos conscientes:\n",
    "\n",
    "* Faltan valores en algunas variables.\n",
    "* Al observar las distribuciones, vimos que ApplicantIncome y LoanAmount parecían contener valores extremos.\n",
    "\n",
    "Además de estos problemas con los campos numéricos, también debemos ver los campos no numéricos, es decir, Género, Área de la propiedad, Casado, Educación y Dependientes para ver, si contienen información útil o incompleta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificación de los valores faltantes\n",
    "\n",
    "Echemos un vistazo a los valores faltantes en todas las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: sum(x.isnull()),axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque los valores perdidos no son muy altos en número, muchas variables los tienen y cada uno de ellos debe estimarse y agregarse en los datos. Es importante tener en cuenta que los valores perdidos pueden no ser siempre NaN o null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cómo completar los valores perdidos en LoanAmount?\n",
    "\n",
    "Existen numerosas formas de completar los valores faltantes del monto del préstamo, siendo el más simple el reemplazo por la media, que se puede hacer mediante el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra alternativa es construir un modelo de aprendizaje supervisado para predecir el valor faltante, en función de valores existentes (las técnicas para hacer esto se discutirán en la siguiente sección).\n",
    "\n",
    "Dado que, el objetivo de esta sección es resaltar los pasos en la limpieza de los datos, adoptaremos un enfoque más simple que el de aprendizaje supervizado.\n",
    "\n",
    "Primero, veamos el Box Plot para ver si existe una tendencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.boxplot(column='LoanAmount', by = ['Education','Self_Employed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible apreciar algunas variaciones en la mediana del monto del préstamo para cada grupo y esto puede usarse para llenar los valores faltantes. Pero primero, debemos asegurarnos de que cada una de las variables de Self_Employed y Education no debe tener valores perdidos. Veamos la tabla de frecuencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como aproximadamente 86% de los valores son \"No\", es seguro llenar los valores faltantes como \"No\", ya que hay una alta probabilidad de éxito. Esto se puede hacer usando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].fillna('No',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, crearemos una tabla dinámica, que nos proporciona valores medios para todos los grupos de valores únicos de las características `Self_Employed` y `Education`. A continuación, definimos una función, que devuelve los valores de estas celdas y la aplica para completar los valores que faltan del monto del préstamo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df.pivot_table(values='LoanAmount', index='Self_Employed' ,columns='Education', aggfunc=np.median)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fage(x):\n",
    "    return table.loc[x['Self_Employed'],x['Education']]\n",
    "\n",
    "df[df['LoanAmount'].isnull()].apply(fage, axis=1)\n",
    "\n",
    "df['LoanAmount'].fillna(df[df['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True)\n",
    "table.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cómo tratar los valores extremos?\n",
    "\n",
    "Analicemos LoanAmount primero. Dado que los valores extremos seguramente no se debe a un error, es factible que algunas personas soliciten préstamos de alto valor debido a necesidades específicas. Entonces, en lugar de tratarlos como valores atípicos, probemos una transformación logarítmica para anular su efecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount_log'] = np.log(df['LoanAmount'])\n",
    "df['LoanAmount_log'].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora la distribución se ve mucho más cerca de una normal (preferible para muchos modelos predictivos) y el efecto de los valores extremos ha disminuido significativamente.\n",
    "\n",
    "Ahora, en relación a `ApplicantIncome`, una intuición puede ser que algunos solicitantes tienen un ingreso más bajo, pero tiene avales fuertes. Por lo tanto, podría ser una buena idea combinar ambos ingresos como ingreso total y tomar una transformación de este valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "df['TotalIncome_log'] = np.log(df['TotalIncome'])\n",
    "df['TotalIncome_log'].hist(bins=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos que la distribución es mucho mejor que antes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3.- Construcción de modelos predictivos\n",
    "\n",
    "Ahora que los datos han sido filtrados, veamos código en Python para crear un modelos predictivos para nuestro conjunto de datos. Skicit-Learn (sklearn) es la biblioteca más utilizada en Python para este propósito. Como sklearn requiere que todas las entradas sean numéricas, debemos convertir todas nuestras variables categóricas en numéricas codificando las categorías. Esto se puede hacer usando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"train.csv\").dropna() # eliminamos las filas con algún valor desconocido para evitar problemas\n",
    "\n",
    "var_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "df.dtypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, importaremos los módulos requeridos, luego definiremos una función de clasificación genérica, que toma un modelo como entrada y determina el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "    scores = cross_val_score(model, data[predictors], data[outcome], cv=10)\n",
    "    \n",
    "    model.fit(data[predictors],data[outcome])\n",
    "    predictions = model.predict(data[predictors])\n",
    "    accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "    \n",
    "    print(\"Rendimiento : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "    print(\"Rendimiento corregido: %s\" % \"{0:.3%}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento reportado, corresponde al porcentaje de acierto del modelo en el mismo conjunto en que se calibró. Dado que esto puede entregar una idea incorrecta sobre el rendimiento real del modelo, se reporta también el rendimiento corregido, que entrega el rendimiento promedio del algoritmo en un conjunto de datos no visto, utilizando [validación cruzada](https://bit.ly/1CHXsNH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión logística\n",
    "\n",
    "Hagamos nuestro primer modelo de Regresión Logística. Una forma sería tomar todas las variables en el modelo, pero esto podría resultar en un ajuste excesivo (para una descripción detallada, ver [acá](https://en.wikipedia.org/wiki/Overfitting)). En palabras simples, tomar todas las variables puede hacer que el modelo comprenda relaciones complejas entre los datos, que realmente no existen (ruido).\n",
    "\n",
    "Podemos hacer fácilmente algunas hipótesis intuitivas para comenzar. Las posibilidades de obtener un préstamo serán mayores para:\n",
    "\n",
    "* Los solicitantes tienen un historial de crédito (lo vimos anteriromente)\n",
    "* Solicitantes con mayores ingresos\n",
    "* Solicitantes con nivel de educación superior\n",
    "* Propiedades en áreas urbanas con altas perspectivas de crecimiento\n",
    "\n",
    "Así que vamos a hacer nuestro primer modelo con `Credit_History`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_var = 'Loan_Status'\n",
    "model = LogisticRegression()\n",
    "predictor_var = ['Credit_History']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, usemos más variables en el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['Credit_History','Education','Married','Self_Employed','Property_Area']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, esperamos que el rendimiento aumente al agregar variables. Pero este es un caso más desafiante, ya que el rendimiento no se ve afectado por esto. Veamos ahora otras técnicas que podrían entregar distintos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árboles de decisión\n",
    "\n",
    "Los árboles de decisión son otro método para construir un modelo predictivo. Generalmente proporciona un mejor rendimiento que el modelo de regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "predictor_var = ['Credit_History','Loan_Amount_Term','LoanAmount']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí observamos que, aunque aumentó el rendimiento, la corrección de este (validación cruzada) bajó. Este es el resultado de un modelo que sobreajusta los datos. Queda como ejercicio buscar nuevas combinaciones de variables que mejoren el rendimiento corregido (o la creación de nuevas variables, como vimos anteriormente)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
